{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset = pd.read_csv('OneDrive/Desktop/Week 3 - Data Cleaning (Pandas) - 2/chipotle.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowing our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking datatypes of each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling missing values\n",
    "As we can see there are around 1400 missing values in choice_description column. choice_description column is a categorical column so we can not replace those null values with mean or median of the column. After studying the dataset, we can see that the there are null values in choice_description column for only those items in item_name for which no choices are available. Therefore, we can replace the null values with 'No Choices' String value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset['choice_description'] = Dataset['choice_description'].fillna('No choice')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling inconsistency in data\n",
    "Now, If we check the dtype of item_price, we can see that it is object because there is dollar sign attached to the price value. If we want to perform some statistical analysis, we have to change the dtype of item_price column to 'float64'.Also we have to remove the dollar sign. We can use the replace method of python to do that. To change the dtype we use 'astype' method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset['item_price'] = Dataset['item_price'].str.replace('$', '').astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing choice_desciption column\n",
    "If we look at the choice_description column, we can clearly see that there are multiple choices for a single item which should be handled properly for accurate analysis. What we can do in this case is create a column for every unique item in choice_description column and mark their presence by 0 or 1 according to their corresponding value in choice_description column just like one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extract_items(choice_description):\n",
    "    if pd.isnull(choice_description):\n",
    "        return []\n",
    "    else:\n",
    "        # Assuming items are enclosed in square brackets\n",
    "        return [item.strip(\" []\") for item in choice_description.split(\",\")]\n",
    "Dataset['parsed_items'] = Dataset['choice_description'].apply(extract_items)\n",
    "unique_items = set(item for sublist in Dataset['parsed_items'] for item in sublist)\n",
    "for item in unique_items:\n",
    "    Dataset[item] = Dataset['parsed_items'].apply(lambda x: 1 if item in x else 0)\n",
    "Dataset.drop(['choice_description', 'parsed_items'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data integrity check for order_id column\n",
    "To cross-reference the Order ID column for integrity, we can check for irregularities or patterns by examining the unique values in the 'order_id' column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique values in the 'order_id' column\n",
    "unique_order_ids = Dataset['order_id'].unique()\n",
    "\n",
    "# Check for irregularities or patterns\n",
    "# We can check if the order IDs follow a specific pattern or have consistent lengths\n",
    "for order_id in unique_order_ids:\n",
    "    print(f\"Order ID: {order_id}, Length: {len(str(order_id))}\")\n",
    "\n",
    "# Additionally, we can check if there are any missing values in the 'order_id' column\n",
    "missing_order_ids = Dataset['order_id'].isnull().sum()\n",
    "print(f\"Number of missing Order IDs: {missing_order_ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Item name standardization\n",
    "To standardize the Item Name column and unify variations for better analysis, we can apply text processing techniques to clean and standardize the item names.\n",
    "The code performs the following operations:\n",
    "1.Converts all item names to lowercase for consistency.\n",
    "2.Removes leading and trailing whitespaces.\n",
    "3.Replaces hyphens with spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset['item_name'] = Dataset['item_name'].str.lower() \n",
    "Dataset['item_name'] = Dataset['item_name'].str.strip() \n",
    "Dataset['item_name'] = Dataset['item_name'].str.replace('-', ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relationship between Quantity and Item_price\n",
    "To perform a data integrity check to ensure that quantities and prices align with the corresponding items and descriptions, we can examine the dataset to identify any discrepancies.This code checks for records where the product of quantity and item price does not match the total price, highlighting potential inconsistencies.This analysis helps to identify records where the calculated total price does not align with the given quantity and item price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset['total_price'] = Dataset['quantity'] * Dataset['item_price']\n",
    "\n",
    "# Check for inconsistencies between quantities and prices\n",
    "inconsistent_records = Dataset[Dataset['total_price'] != Dataset['total_price']]\n",
    "\n",
    "# Display inconsistent records\n",
    "print(\"Inconsistent Records:\")\n",
    "print(inconsistent_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
